{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n__print__ = print\ndef print(string):\n    os.system(f'echo \\\"{string}\\\"')\n    __print__(string)","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install pytorch-transformers","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting pytorch-transformers\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n\u001b[K     |████████████████████████████████| 184kB 2.9MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (1.9.212)\nCollecting sacremoses (from pytorch-transformers)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n\u001b[K     |████████████████████████████████| 890kB 18.2MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (4.32.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (2.22.0)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (1.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (1.17.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (0.1.83)\nRequirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (2019.8.19)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-transformers) (0.9.4)\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-transformers) (0.2.1)\nRequirement already satisfied: botocore<1.13.0,>=1.12.212 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-transformers) (1.12.212)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->pytorch-transformers) (1.12.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->pytorch-transformers) (7.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->pytorch-transformers) (0.13.2)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-transformers) (1.24.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-transformers) (2019.6.16)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-transformers) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-transformers) (3.0.4)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.212->boto3->pytorch-transformers) (2.8.0)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.212->boto3->pytorch-transformers) (0.15.2)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=ec36a2301d5c0dace479ecb2db426c0f831506cc6ea71dfa75096a5d5e9f4115\n  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\nSuccessfully built sacremoses\nInstalling collected packages: sacremoses, pytorch-transformers\nSuccessfully installed pytorch-transformers-1.2.0 sacremoses-0.0.43\n","name":"stdout"}]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:47.468423Z","start_time":"2019-09-02T13:42:46.505711Z"},"trusted":true},"cell_type":"code","source":"from fastai.text import *\nfrom fastai.metrics import *\nfrom pytorch_transformers import RobertaTokenizer","execution_count":3,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:47.474401Z","start_time":"2019-09-02T13:42:47.470059Z"},"trusted":true},"cell_type":"code","source":"# Creating a config object to store task specific information\nclass Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n        \nconfig = Config(\n    testing=False,\n    seed = 2019,\n    roberta_model_name='roberta-base', # can also be exchnaged with roberta-large \n    max_lr=1e-5,\n    epochs=1,\n    use_fp16=False,\n    bs=4, \n    max_seq_len=256, \n    num_labels = 2,\n    hidden_dropout_prob=.05,\n    hidden_size=768, # 1024 for roberta-large\n    start_tok = \"<s>\",\n    end_tok = \"</s>\",\n)","execution_count":4,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:47.913197Z","start_time":"2019-09-02T13:42:47.475714Z"},"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","execution_count":5,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:47.917596Z","start_time":"2019-09-02T13:42:47.914436Z"},"trusted":true},"cell_type":"code","source":"if config.testing: df = df[:5000]\nprint(df.shape)","execution_count":6,"outputs":[{"output_type":"stream","text":"(50000, 2)\n","name":"stdout"}]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:47.934996Z","start_time":"2019-09-02T13:42:47.918828Z"},"trusted":true},"cell_type":"code","source":"df.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:47.938328Z","start_time":"2019-09-02T13:42:47.936287Z"},"trusted":true},"cell_type":"code","source":"feat_cols = \"review\"\nlabel_cols = \"sentiment\"","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting Up the Tokenizer"},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:47.944376Z","start_time":"2019-09-02T13:42:47.940082Z"},"trusted":true},"cell_type":"code","source":"class FastAiRobertaTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): \n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len \n    def __call__(self, *args, **kwargs): \n        return self \n    def tokenizer(self, t:str) -> List[str]: \n        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]","execution_count":9,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:48.296371Z","start_time":"2019-09-02T13:42:47.945766Z"},"trusted":true},"cell_type":"code","source":"# create fastai tokenizer for roberta\nroberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\nfastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), \n                             pre_rules=[], post_rules=[])","execution_count":10,"outputs":[{"output_type":"stream","text":"100%|██████████| 898823/898823 [00:00<00:00, 5767119.08B/s]\n100%|██████████| 456318/456318 [00:00<00:00, 3129203.13B/s]\n","name":"stderr"}]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:48.407892Z","start_time":"2019-09-02T13:42:48.297631Z"},"trusted":true},"cell_type":"code","source":"# create fastai vocabulary for roberta\npath = Path()\nroberta_tok.save_vocabulary(path)\n\nwith open('vocab.json', 'r') as f:\n    roberta_vocab_dict = json.load(f)\n    \nfastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))","execution_count":11,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:48.41299Z","start_time":"2019-09-02T13:42:48.409165Z"},"trusted":true},"cell_type":"code","source":"# Setting up pre-processors\nclass RobertaTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n\nclass RobertaNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=fastai_roberta_vocab, **kwargs)\n\n\ndef get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    \"\"\"\n    Constructing preprocessors for Roberta\n    We remove sos and eos tokens since we add that ourselves in the tokenizer.\n    We also use a custom vocabulary to match the numericalization with the original Roberta model.\n    \"\"\"\n    return [RobertaTokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(vocab=vocab)]","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting up the DataBunch"},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:48.421076Z","start_time":"2019-09-02T13:42:48.41423Z"},"trusted":true},"cell_type":"code","source":"# Creating a Roberta specific DataBunch class\nclass RobertaDataBunch(TextDataBunch):\n    \"Create a `TextDataBunch` suitable for training Roberta\"\n    @classmethod\n    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,\n               pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, \n               dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch:\n        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n        val_bs = ifnone(val_bs, bs)\n        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs)\n        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n        dataloaders = [train_dl]\n        for ds in datasets[1:]:\n            lengths = [len(t) for t in ds.x.items]\n            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n        return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)","execution_count":13,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:48.428024Z","start_time":"2019-09-02T13:42:48.422267Z"},"trusted":true},"cell_type":"code","source":"class RobertaTextList(TextList):\n    _bunch = RobertaDataBunch\n    _label_cls = TextList","execution_count":14,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:42:51.659205Z","start_time":"2019-09-02T13:42:48.429219Z"},"trusted":true},"cell_type":"code","source":"# loading the tokenizer and vocab processors\nprocessor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n\n# creating our databunch \ndata = RobertaTextList.from_df(df, \".\", cols=feat_cols, processor=processor) \\\n    .split_by_rand_pct(seed=config.seed) \\\n    .label_from_df(cols=label_cols,label_cls=CategoryList) \\\n    .databunch(bs=config.bs, pad_first=False, pad_idx=0)","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the Model"},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:49:09.101481Z","start_time":"2019-09-02T13:49:09.094984Z"},"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom pytorch_transformers import RobertaModel\n\n# defining our model architecture \nclass CustomRobertaModel(nn.Module):\n    def __init__(self,num_labels=2):\n        super(CustomRobertaModel,self).__init__()\n        self.num_labels = num_labels\n        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer\n        \n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # \n        logits = self.classifier(pooled_output)        \n        return logits","execution_count":16,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T13:49:13.411872Z","start_time":"2019-09-02T13:49:09.914667Z"},"trusted":true},"cell_type":"code","source":"roberta_model = CustomRobertaModel()\n\nlearn = Learner(data, roberta_model, metrics=[accuracy])","execution_count":17,"outputs":[{"output_type":"stream","text":"100%|██████████| 481/481 [00:00<00:00, 136379.38B/s]\n100%|██████████| 501200538/501200538 [00:13<00:00, 36838272.82B/s]\n","name":"stderr"}]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T14:20:31.006732Z","start_time":"2019-09-02T14:14:45.072892Z"},"trusted":true},"cell_type":"code","source":"learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\nlearn.fit_one_cycle(config.epochs, max_lr=config.max_lr)","execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.200836</td>\n      <td>0.155621</td>\n      <td>0.940600</td>\n      <td>23:01</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T14:11:19.976614Z","start_time":"2019-09-02T14:11:19.970844Z"},"trusted":true},"cell_type":"code","source":"#predictions\ndef get_preds_as_nparray(ds_type) -> np.ndarray:\n    learn.model.roberta.eval()\n    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in data.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    ordered_preds = preds[reverse_sampler, :]\n    pred_values = np.argmax(ordered_preds, axis=1)\n    return ordered_preds, pred_values","execution_count":19,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T14:11:40.8352Z","start_time":"2019-09-02T14:11:20.102674Z"},"trusted":true},"cell_type":"code","source":"preds, pred_values = get_preds_as_nparray(DatasetType.Valid)","execution_count":20,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-09-02T14:11:40.840225Z","start_time":"2019-09-02T14:11:40.837056Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"# accuracy on validation set\n(pred_values == data.valid_ds.y.items).mean()","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"0.9406"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}