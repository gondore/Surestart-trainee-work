{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gondore/Surestart-trainee-work/blob/main/experimental_dqn_nsdc_crafter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UvFK2pCKTLIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5734617a-3ebd-4d98-8bbd-10f6e225465d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: crafter in /usr/local/lib/python3.10/dist-packages (1.8.3)\n",
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: shimmy[gym-v26] in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.9.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from crafter) (2.31.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from crafter) (9.4.0)\n",
            "Requirement already satisfied: opensimplex in /usr/local/lib/python3.10/dist-packages (from crafter) (0.4.5)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.10/dist-packages (from crafter) (0.18.6)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: gym>=0.26.2 in /usr/local/lib/python3.10/dist-packages (from shimmy[gym-v26]) (0.26.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.26.2->shimmy[gym-v26]) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.4)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml->crafter) (0.2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install gymnasium crafter stable_baselines3 shimmy[gym-v26]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7-F_xxAn3gJq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p-synYW9TA7t",
        "collapsed": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rpiA-m5-TSAC"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import crafter\n",
        "\n",
        "\n",
        "#env = gym.make(\"GymV26Environment-v0\", env_id = 'CrafterReward-v1')  # Or CrafterNoReward-v1\n",
        "env = crafter.Env()\n",
        "env = crafter.Recorder(\n",
        "  env, './path/to/logdir',\n",
        "  save_stats=True,\n",
        "  save_video=False,\n",
        "  save_episode=False,\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9buj8vIpXRZ7"
      },
      "source": [
        "It looks like our observation space is image data? 64x64x3 (height x width x channel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uvXfkOv2XMJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e192d8b-a6ff-4269-c19a-9d88a02f0390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Space Box(0, 255, (64, 64, 3), uint8)\n"
          ]
        }
      ],
      "source": [
        "print(\"Observation Space\", env.observation_space)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8r6krvL1TT1f"
      },
      "outputs": [],
      "source": [
        "observation_space = env.observation_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imcEtApmTfSS"
      },
      "source": [
        "According to crafter documentation, (https://github.com/danijar/crafter/blob/e04542a2159f1aad3d4c5ad52e8185717380ee3a/crafter/env.py#L15)\n",
        "\n",
        "```\n",
        "  def observation_space(self):\n",
        "    return BoxSpace(0, 255, tuple(self._size) + (3,), np.uint8)\n",
        "```\n",
        "\n",
        " I believe the observation space is in the form of BoxSpace, which I'm not familiar with. This plot below is just a quick script that visualizes the observation space. I think we will have to work with image data and flatten it to state representations?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IwQcCg39TVPl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf      # Deep Learning library\n",
        "import numpy as np           # Handle matrices\n",
        "\n",
        "\n",
        "from skimage import transform # Help us to preprocess the frames\n",
        "from skimage.color import rgb2gray # Help us to gray our frames\n",
        "\n",
        "import matplotlib.pyplot as plt # Display graphs\n",
        "\n",
        "from collections import deque# Ordered collection with ends\n",
        "\n",
        "import random\n",
        "\n",
        "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-9inXSNXT8T9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28d91e0-64c0-4791-b6c6-8274ed668ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The action size is :  17\n"
          ]
        }
      ],
      "source": [
        "print(\"The action size is : \", env.action_space.n)\n",
        "# one hot encode actions\n",
        "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V80Yl8BGh_nq"
      },
      "source": [
        "**Preprocessing observationspace**\n",
        "- grayscale frames\n",
        "- normalizing pixel values\n",
        "- resize preprocessed frame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2Ijgwn-DRtTE"
      },
      "outputs": [],
      "source": [
        "def preprocess_frame(frame):\n",
        "    # Greyscale frame\n",
        "    gray = rgb2gray(frame)\n",
        "\n",
        "    # Normalize Pixel Values\n",
        "    normalized_frame = frame/255.0\n",
        "\n",
        "    # Resize\n",
        "    # Thanks to Miko≈Çaj Walkowiak\n",
        "    preprocessed_frame = transform.resize(normalized_frame, [110,84])\n",
        "\n",
        "    return preprocessed_frame # 110x84x1 frame"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**stack_frames**"
      ],
      "metadata": {
        "id": "XRBakch59anp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P-rRSQkRh-dL"
      },
      "outputs": [],
      "source": [
        "stack_size = 4 # We stack 4 frames\n",
        "\n",
        "# Initialize deque with zero-images one array for each image\n",
        "stacked_frames  =  deque([np.zeros((110,84), dtype=int) for i in range(stack_size)], maxlen=4)\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "\n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros((110,84), dtype=int) for i in range(stack_size)], maxlen=4)\n",
        "\n",
        "        # Because we're in a new episode, copy the same frame 4x\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "\n",
        "    else:\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "\n",
        "    return stacked_state, stacked_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRuEUQojjBzs"
      },
      "source": [
        "**Hyperparameter setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pgxpFsnei-ZL"
      },
      "outputs": [],
      "source": [
        "### MODEL HYPERPARAMETERS\n",
        "state_size = [110, 84, 4]      # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels)\n",
        "action_size = env.action_space.n # 8 possible actions\n",
        "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
        "\n",
        "### TRAINING HYPERPARAMETERS\n",
        "total_episodes = 50            # Total episodes for training\n",
        "max_steps = 50000              # Max possible steps in an episode\n",
        "batch_size = 64                # Batch size\n",
        "\n",
        "# Exploration parameters for epsilon greedy strategy\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability\n",
        "decay_rate = 0.00001           # exponential decay rate for exploration prob\n",
        "\n",
        "# Q learning hyperparameters\n",
        "gamma = 0.9                    # Discounting rate\n",
        "\n",
        "### MEMORY HYPERPARAMETERS\n",
        "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
        "memory_size = 1000000          # Number of experiences the Memory can keep\n",
        "\n",
        "### PREPROCESSING HYPERPARAMETERS\n",
        "stack_size = 4                 # Number of frames stacked\n",
        "\n",
        "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
        "training = False\n",
        "\n",
        "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
        "episode_render = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DQN implementation**"
      ],
      "metadata": {
        "id": "lgJD_rjP97E1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sWavPV5CRm1l"
      },
      "outputs": [],
      "source": [
        "class DQNetwork:\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        with tf.compat.v1.variable_scope(name):\n",
        "            # We create the placeholders\n",
        "            # *state_size means that we take each element of state_size in tuple hence is like if we wrote\n",
        "            # [None, 84, 84, 4]\n",
        "            self.inputs_ = tf.keras.Input(shape=state_size, dtype=tf.float32, name=\"inputs\")\n",
        "            self.actions_ = tf.keras.Input(shape=(self.action_size,), dtype=tf.float32, name=\"actions_\")\n",
        "\n",
        "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
        "            self.target_Q = tf.keras.Input(shape=(1,), dtype=tf.float32, name=\"target\")\n",
        "\n",
        "            \"\"\"\n",
        "            First convnet:\n",
        "            CNN\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            # Input is 110x84x4\n",
        "            self.conv1 = tf.keras.layers.Conv2D(filters=32,\n",
        "                                                kernel_size=(8, 8),\n",
        "                                                strides=(4, 4),\n",
        "                                                padding=\"VALID\",\n",
        "                                                kernel_initializer=tf.initializers.GlorotUniform(),\n",
        "                                                name=\"conv1\")\n",
        "            self.conv1_out = tf.keras.layers.ELU(name=\"conv1_out\")\n",
        "\n",
        "            \"\"\"\n",
        "            Second convnet:\n",
        "            CNN\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            self.conv2 = tf.keras.layers.Conv2D(filters=64,\n",
        "                                                kernel_size=(4, 4),\n",
        "                                                strides=(2, 2),\n",
        "                                                padding=\"VALID\",\n",
        "                                                kernel_initializer=tf.initializers.GlorotUniform(),\n",
        "                                                name=\"conv2\")\n",
        "            self.conv2_out = tf.keras.layers.ELU(name=\"conv2_out\")\n",
        "\n",
        "            \"\"\"\n",
        "            Third convnet:\n",
        "            CNN\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            self.conv3 = tf.keras.layers.Conv2D(filters=64,\n",
        "                                                kernel_size=(3, 3),\n",
        "                                                strides=(2, 2),\n",
        "                                                padding=\"VALID\",\n",
        "                                                kernel_initializer=tf.initializers.GlorotUniform(),\n",
        "                                                name=\"conv3\")\n",
        "            self.conv3_out = tf.keras.layers.ELU(name=\"conv3_out\")\n",
        "\n",
        "            self.flatten = tf.keras.layers.Flatten()\n",
        "            self.fc = tf.keras.layers.Dense(units=512,\n",
        "                                             activation=tf.keras.activations.elu,\n",
        "                                             kernel_initializer=tf.initializers.GlorotUniform(),\n",
        "                                             name=\"fc1\")\n",
        "            self.output = tf.keras.layers.Dense(units=self.action_size,\n",
        "                                                 activation=None,\n",
        "                                                 kernel_initializer=tf.initializers.GlorotUniform(),\n",
        "                                                 name=\"output\")\n",
        "\n",
        "            # Connect the layers\n",
        "            x = self.conv1(self.inputs_)\n",
        "            x = self.conv1_out(x)\n",
        "            x = self.conv2(x)\n",
        "            x = self.conv2_out(x)\n",
        "            x = self.conv3(x)\n",
        "            x = self.conv3_out(x)\n",
        "            x = self.flatten(x)\n",
        "            x = self.fc(x)\n",
        "            self.output_ = self.output(x)\n",
        "\n",
        "            # Q is our predicted Q value.\n",
        "            self.Q = tf.reduce_sum(tf.multiply(self.output_, self.actions_))\n",
        "\n",
        "            # The loss is the difference between our predicted Q_values and the Q_target\n",
        "            # Sum(Qtarget - Q)^2\n",
        "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
        "\n",
        "            # Get the list of trainable variables\n",
        "            self.trainable_variables = self.conv1.trainable_variables + \\\n",
        "                                        self.conv1_out.trainable_variables + \\\n",
        "                                        self.conv2.trainable_variables + \\\n",
        "                                        self.conv2_out.trainable_variables + \\\n",
        "                                        self.conv3.trainable_variables + \\\n",
        "                                        self.conv3_out.trainable_variables + \\\n",
        "                                        self.fc.trainable_variables + \\\n",
        "                                        self.output.trainable_variables\n",
        "\n",
        "            # Create an optimizer\n",
        "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "    def train(self, inputs, actions, target_Q):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            predicted_Q = self(inputs)\n",
        "            # Calculate the loss\n",
        "            loss = tf.reduce_mean(tf.square(target_Q - predicted_Q))\n",
        "\n",
        "        # Get gradients of loss w.r.t. the trainable variables\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        # Apply gradients using the optimizer\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5vtI06UADdC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ozn11f9wjjAT"
      },
      "outputs": [],
      "source": [
        "# Reset the graph\n",
        "tf.compat.v1.reset_default_graph()\n",
        "# Instantiate the DQNetwork\n",
        "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4VeG6_axjn_p"
      },
      "outputs": [],
      "source": [
        "class Memory():\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen = max_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        buffer_size = len(self.buffer)\n",
        "        index = np.random.choice(np.arange(buffer_size),\n",
        "                                size = batch_size,\n",
        "                                replace = False)\n",
        "\n",
        "        return [self.buffer[i] for i in index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "awdR8DKQmG-R"
      },
      "outputs": [],
      "source": [
        "# Instantiate memory\n",
        "memory = Memory(max_size = memory_size)\n",
        "for i in range(pretrain_length):\n",
        "    # If it's the first step\n",
        "    if i == 0:\n",
        "        state = env.reset()\n",
        "\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "    # Get the next_state, the rewards, done by taking a random action\n",
        "    choice = random.randint(1,len(possible_actions))-1\n",
        "    action = possible_actions[choice]\n",
        "    next_state, reward, done, _ = env.step(np.argmax(action))\n",
        "\n",
        "    #env.render()\n",
        "\n",
        "    # Stack the frames\n",
        "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "\n",
        "    # If the episode is finished (we're dead 3x)\n",
        "    if done:\n",
        "        # We finished the episode\n",
        "        next_state = np.zeros(state.shape)\n",
        "\n",
        "        # Add experience to memory\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "        # Start a new episode\n",
        "        state = env.reset()\n",
        "\n",
        "        # Stack the frames\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "    else:\n",
        "        # Add experience to memory\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "        # Our new state is now the next_state\n",
        "        state = next_state"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vq6hl_I8GFfz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup TensorBoard Writer\n",
        "#writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
        "\n",
        "## Losses\n",
        "#tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
        "\n",
        "#write_op = tf.summary.merge_all()"
      ],
      "metadata": {
        "id": "f9nr2-omA3jb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "6274057b-f3a9-4db7-e27d-24cd647e267a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorboard.summary._tf.summary' has no attribute 'FileWriter'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-dbb167f63461>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Setup TensorBoard Writer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tensorboard/dqn/1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## Losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDQNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorboard.summary._tf.summary' has no attribute 'FileWriter'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
        "    ## EPSILON GREEDY STRATEGY\n",
        "    # Choose action a from state s using epsilon greedy.\n",
        "    ## First we randomize a number\n",
        "    exp_exp_tradeoff = np.random.rand()\n",
        "\n",
        "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
        "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
        "\n",
        "    if (explore_probability > exp_exp_tradeoff):\n",
        "        # Make a random action (exploration)\n",
        "        choice = random.randint(1,len(possible_actions))-1\n",
        "        action = possible_actions[choice]\n",
        "\n",
        "    else:\n",
        "        # Get action from Q-network (exploitation)\n",
        "        # Estimate the Qs values state\n",
        "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
        "\n",
        "        # Take the biggest Q value (= the best action)\n",
        "        choice = np.argmax(Qs)\n",
        "        action = possible_actions[choice]\n",
        "\n",
        "\n",
        "    return action, explore_probability"
      ],
      "metadata": {
        "id": "EMnTGJJHDdXM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saver will help us to save our model\n",
        "#saver = tf.train.Saver()\n",
        "\n",
        "if training == True:\n",
        "\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        # Initialize the variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # Initialize the decay rate (that will use to reduce epsilon)\n",
        "        decay_step = 0\n",
        "\n",
        "        for episode in range(total_episodes):\n",
        "            # Set step to 0\n",
        "            step = 0\n",
        "\n",
        "            # Initialize the rewards of the episode\n",
        "            episode_rewards = []\n",
        "\n",
        "            # Make a new episode and observe the first state\n",
        "            state = env.reset()\n",
        "\n",
        "            # Remember that stack frame function also call our preprocess function.\n",
        "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "            while step < max_steps:\n",
        "                step += 1\n",
        "\n",
        "                #Increase decay_step\n",
        "                decay_step +=1\n",
        "\n",
        "                # Predict the action to take and take it\n",
        "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
        "\n",
        "                #Perform the action and get the next_state, reward, and done information\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                if episode_render:\n",
        "                    env.render()\n",
        "\n",
        "                # Add the reward to total reward\n",
        "                episode_rewards.append(reward)\n",
        "\n",
        "                # If the game is finished\n",
        "                if done:\n",
        "                    # The episode ends so no next state\n",
        "                    next_state = np.zeros((110,84), dtype=np.int)\n",
        "\n",
        "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "                    # Set step = max_steps to end the episode\n",
        "                    step = max_steps\n",
        "\n",
        "                    # Get the total reward of the episode\n",
        "                    total_reward = np.sum(episode_rewards)\n",
        "\n",
        "                    print('Episode: {}'.format(episode),\n",
        "                                  'Total reward: {}'.format(total_reward),\n",
        "                                  'Explore P: {:.4f}'.format(explore_probability),\n",
        "                                'Training Loss {:.4f}'.format(loss))\n",
        "\n",
        "                    rewards_list.append((episode, total_reward))\n",
        "\n",
        "                    # Store transition <st,at,rt+1,st+1> in memory D\n",
        "                    memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "                else:\n",
        "                    # Stack the frame of the next_state\n",
        "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "                    # Add experience to memory\n",
        "                    memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "                    # st+1 is now our current state\n",
        "                    state = next_state\n",
        "\n",
        "\n",
        "                ### LEARNING PART\n",
        "                # Obtain random mini-batch from memory\n",
        "                batch = memory.sample(batch_size)\n",
        "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
        "                actions_mb = np.array([each[1] for each in batch])\n",
        "                rewards_mb = np.array([each[2] for each in batch])\n",
        "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
        "                dones_mb = np.array([each[4] for each in batch])\n",
        "\n",
        "                target_Qs_batch = []\n",
        "\n",
        "                # Get Q values for next_state\n",
        "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
        "\n",
        "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
        "                for i in range(0, len(batch)):\n",
        "                    terminal = dones_mb[i]\n",
        "\n",
        "                    # If we are in a terminal state, only equals reward\n",
        "                    if terminal:\n",
        "                        target_Qs_batch.append(rewards_mb[i])\n",
        "\n",
        "                    else:\n",
        "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
        "                        target_Qs_batch.append(target)\n",
        "\n",
        "\n",
        "                targets_mb = np.array([each for each in target_Qs_batch])\n",
        "\n",
        "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
        "                                        feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                                   DQNetwork.target_Q: targets_mb,\n",
        "                                                   DQNetwork.actions_: actions_mb})\n",
        "\n",
        "                # Write TF Summaries\n",
        "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                                       DQNetwork.target_Q: targets_mb,\n",
        "                                                       DQNetwork.actions_: actions_mb})\n",
        "                writer.add_summary(summary, episode)\n",
        "                writer.flush()\n",
        "\n",
        "            # Save model every 5 episodes\n",
        "            if episode % 5 == 0:\n",
        "                save_path = model.save(\"./models/model.ckpt\")\n",
        "                print(\"Model Saved\")"
      ],
      "metadata": {
        "id": "_x23S3r6DjRb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium torch numpy matplotlib\n"
      ],
      "metadata": {
        "id": "MaHzGWFoGPvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac51ce4-53cf-4435-dd6d-5cca713a889b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.9.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "class ConvDQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ConvDQN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=8, stride=4),  # input channels = 3 for RGB images\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(64 * 4 * 4, 512),  # This needs to be adjusted based on the output size of the conv layers\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 3, 1, 2)  # Change image format from HWC to CHW expected by PyTorch\n",
        "        conv_out = self.conv_layers(x).reshape(x.size(0), -1)  # Flatten the output for the FC layers\n",
        "        return self.fc_layers(conv_out)\n",
        "\n"
      ],
      "metadata": {
        "id": "h8E4jDwKt-1n"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.array(state), action, reward, np.array(next_state), done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "4hp2Z4hpuGyk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, replay_buffer):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.model = ConvDQN(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters())\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.epsilon = 0.99\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.gamma = 0.99\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() > self.epsilon:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = self.model(state)\n",
        "            action = q_values.max(1)[1].item()\n",
        "        else:\n",
        "            action = random.randrange(self.action_dim)\n",
        "        return action\n",
        "\n",
        "    def train(self, batch_size):\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
        "        states = torch.FloatTensor(states)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_values = self.model(next_states).max(1)[0]\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        loss = self.criterion(q_values, expected_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ],
      "metadata": {
        "id": "Ie3F4ZSNuKzy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dim = np.prod(env.observation_space.shape)  # This ensures a flat vector input size is correctly identified\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "#state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "replay_buffer = ReplayBuffer(10000)\n",
        "agent = DQNAgent(state_dim, action_dim, replay_buffer)\n",
        "\n",
        "def train_dqn(episodes):\n",
        "    rewards = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        while True:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            replay_buffer.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            agent.train(32)\n",
        "            if done:\n",
        "                break\n",
        "        rewards.append(episode_reward)\n",
        "        print(f'Episode: {episode}, Reward: {episode_reward}')\n",
        "        agent.epsilon = max(agent.epsilon_min, agent.epsilon_decay * agent.epsilon)  # Decay epsilon\n",
        "    return rewards\n",
        "\n",
        "episodes = 500\n",
        "rewards = train_dqn(episodes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwiww--0uLXR",
        "outputId": "e2a644a9-aac8-472f-bc52-c68df3e52360"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Reward: 0.10000000000000003\n",
            "Episode: 1, Reward: 0.10000000000000003\n",
            "Episode: 2, Reward: 0.10000000000000003\n",
            "Episode: 3, Reward: 0.09999999999999998\n",
            "Episode: 4, Reward: 0.10000000000000003\n",
            "Episode: 5, Reward: 1.1\n",
            "Episode: 6, Reward: 1.1000000000000003\n",
            "Episode: 7, Reward: 0.10000000000000003\n",
            "Episode: 8, Reward: 2.0999999999999996\n",
            "Episode: 9, Reward: 0.10000000000000009\n",
            "Episode: 10, Reward: 5.099999999999999\n",
            "Episode: 11, Reward: 4.1000000000000005\n",
            "Episode: 12, Reward: 3.099999999999999\n",
            "Episode: 13, Reward: 3.099999999999999\n",
            "Episode: 14, Reward: 2.0999999999999996\n",
            "Episode: 15, Reward: 3.0999999999999996\n",
            "Episode: 16, Reward: 1.1000000000000003\n",
            "Episode: 17, Reward: 1.1000000000000003\n",
            "Episode: 18, Reward: 1.1000000000000003\n",
            "Episode: 19, Reward: 3.099999999999999\n",
            "Episode: 20, Reward: 1.1\n",
            "Episode: 21, Reward: 1.1\n",
            "Episode: 22, Reward: 3.099999999999999\n",
            "Episode: 23, Reward: 2.0999999999999996\n",
            "Episode: 24, Reward: 1.1\n",
            "Episode: 25, Reward: 2.099999999999999\n",
            "Episode: 26, Reward: 1.1\n",
            "Episode: 27, Reward: 1.1\n",
            "Episode: 28, Reward: 2.099999999999999\n",
            "Episode: 29, Reward: 1.1000000000000003\n",
            "Episode: 30, Reward: 2.0999999999999996\n",
            "Episode: 31, Reward: 1.1\n",
            "Episode: 32, Reward: 2.099999999999999\n",
            "Episode: 33, Reward: 2.0999999999999996\n",
            "Episode: 34, Reward: 2.099999999999999\n",
            "Episode: 35, Reward: 2.0999999999999996\n",
            "Episode: 36, Reward: 2.1000000000000005\n",
            "Episode: 37, Reward: 1.1\n",
            "Episode: 38, Reward: 3.099999999999999\n",
            "Episode: 39, Reward: 1.0999999999999996\n",
            "Episode: 40, Reward: 3.099999999999999\n",
            "Episode: 41, Reward: 1.1000000000000003\n",
            "Episode: 42, Reward: 2.0999999999999996\n",
            "Episode: 43, Reward: 2.099999999999999\n",
            "Episode: 44, Reward: 2.099999999999999\n",
            "Episode: 45, Reward: 2.099999999999999\n",
            "Episode: 46, Reward: 2.0999999999999996\n",
            "Episode: 47, Reward: 1.1000000000000003\n",
            "Episode: 48, Reward: 1.1000000000000003\n",
            "Episode: 49, Reward: 2.1\n",
            "Episode: 50, Reward: 2.099999999999999\n",
            "Episode: 51, Reward: 2.0999999999999996\n",
            "Episode: 52, Reward: 2.0999999999999996\n",
            "Episode: 53, Reward: 2.099999999999999\n",
            "Episode: 54, Reward: 1.1\n",
            "Episode: 55, Reward: 2.0999999999999996\n",
            "Episode: 56, Reward: 2.099999999999999\n",
            "Episode: 57, Reward: 2.0999999999999996\n",
            "Episode: 58, Reward: 1.1\n",
            "Episode: 59, Reward: 1.1000000000000003\n",
            "Episode: 60, Reward: 2.099999999999999\n",
            "Episode: 61, Reward: 1.1\n",
            "Episode: 62, Reward: 1.1\n",
            "Episode: 63, Reward: 2.099999999999999\n",
            "Episode: 64, Reward: 1.1000000000000003\n",
            "Episode: 65, Reward: 1.1\n",
            "Episode: 66, Reward: 2.0999999999999996\n",
            "Episode: 67, Reward: 1.1\n",
            "Episode: 68, Reward: 1.1\n",
            "Episode: 69, Reward: 2.099999999999999\n",
            "Episode: 70, Reward: 1.1\n",
            "Episode: 71, Reward: 1.1000000000000003\n",
            "Episode: 72, Reward: 1.1\n",
            "Episode: 73, Reward: 2.1\n",
            "Episode: 74, Reward: 1.1000000000000003\n",
            "Episode: 75, Reward: 2.099999999999999\n",
            "Episode: 76, Reward: 2.099999999999999\n",
            "Episode: 77, Reward: 2.099999999999999\n",
            "Episode: 78, Reward: 2.099999999999999\n",
            "Episode: 79, Reward: 1.1000000000000003\n",
            "Episode: 80, Reward: 3.0999999999999996\n",
            "Episode: 81, Reward: 2.099999999999999\n",
            "Episode: 82, Reward: 2.099999999999999\n",
            "Episode: 83, Reward: 1.1000000000000003\n",
            "Episode: 84, Reward: 2.099999999999999\n",
            "Episode: 85, Reward: 2.099999999999999\n",
            "Episode: 86, Reward: 2.099999999999999\n",
            "Episode: 87, Reward: 1.1000000000000003\n",
            "Episode: 88, Reward: 2.0999999999999996\n",
            "Episode: 89, Reward: 1.1000000000000003\n",
            "Episode: 90, Reward: 1.1000000000000003\n",
            "Episode: 91, Reward: 1.1000000000000003\n",
            "Episode: 92, Reward: 1.1000000000000003\n",
            "Episode: 93, Reward: 1.1000000000000003\n",
            "Episode: 94, Reward: 1.1\n",
            "Episode: 95, Reward: 1.1\n",
            "Episode: 96, Reward: 2.099999999999999\n",
            "Episode: 97, Reward: 1.1\n",
            "Episode: 98, Reward: 1.1000000000000003\n",
            "Episode: 99, Reward: 3.099999999999999\n",
            "Episode: 100, Reward: 2.099999999999999\n",
            "Episode: 101, Reward: 2.099999999999999\n",
            "Episode: 102, Reward: 1.1\n",
            "Episode: 103, Reward: 1.1000000000000003\n",
            "Episode: 104, Reward: 1.1000000000000003\n",
            "Episode: 105, Reward: 2.099999999999999\n",
            "Episode: 106, Reward: 0.09999999999999992\n",
            "Episode: 107, Reward: 1.1\n",
            "Episode: 108, Reward: 4.099999999999998\n",
            "Episode: 109, Reward: 1.1000000000000003\n",
            "Episode: 110, Reward: 2.0999999999999996\n",
            "Episode: 111, Reward: 3.099999999999999\n",
            "Episode: 112, Reward: 1.1000000000000003\n",
            "Episode: 113, Reward: 1.1\n",
            "Episode: 114, Reward: 1.1000000000000003\n",
            "Episode: 115, Reward: 1.1\n",
            "Episode: 116, Reward: 1.1\n",
            "Episode: 117, Reward: 0.10000000000000009\n",
            "Episode: 118, Reward: 1.1000000000000003\n",
            "Episode: 119, Reward: 1.1\n",
            "Episode: 120, Reward: 1.1\n",
            "Episode: 121, Reward: 1.1000000000000003\n",
            "Episode: 122, Reward: 1.1\n",
            "Episode: 123, Reward: 2.1\n",
            "Episode: 124, Reward: 3.099999999999999\n",
            "Episode: 125, Reward: 1.0999999999999994\n",
            "Episode: 126, Reward: 2.1000000000000005\n",
            "Episode: 127, Reward: 1.1\n",
            "Episode: 128, Reward: 1.0999999999999999\n",
            "Episode: 129, Reward: 1.1000000000000003\n",
            "Episode: 130, Reward: 2.0999999999999996\n",
            "Episode: 131, Reward: 2.1000000000000005\n",
            "Episode: 132, Reward: 1.1\n",
            "Episode: 133, Reward: 3.099999999999999\n",
            "Episode: 134, Reward: 2.099999999999999\n",
            "Episode: 135, Reward: 1.1000000000000003\n",
            "Episode: 136, Reward: 2.0999999999999996\n",
            "Episode: 137, Reward: 1.1000000000000003\n",
            "Episode: 138, Reward: 3.099999999999999\n",
            "Episode: 139, Reward: 1.0999999999999999\n",
            "Episode: 140, Reward: 2.099999999999999\n",
            "Episode: 141, Reward: 1.0999999999999999\n",
            "Episode: 142, Reward: 2.099999999999999\n",
            "Episode: 143, Reward: 1.1000000000000003\n",
            "Episode: 144, Reward: 1.1000000000000003\n",
            "Episode: 145, Reward: 1.1000000000000003\n",
            "Episode: 146, Reward: 1.0999999999999996\n",
            "Episode: 147, Reward: 2.099999999999999\n",
            "Episode: 148, Reward: 2.099999999999999\n",
            "Episode: 149, Reward: 1.0999999999999999\n",
            "Episode: 150, Reward: 3.099999999999999\n",
            "Episode: 151, Reward: 2.099999999999999\n",
            "Episode: 152, Reward: 2.0999999999999996\n",
            "Episode: 153, Reward: 2.1\n",
            "Episode: 154, Reward: 2.0999999999999996\n",
            "Episode: 155, Reward: 2.0999999999999996\n",
            "Episode: 156, Reward: 1.1000000000000003\n",
            "Episode: 157, Reward: 2.0999999999999996\n",
            "Episode: 158, Reward: 2.099999999999999\n",
            "Episode: 159, Reward: 1.1000000000000003\n",
            "Episode: 160, Reward: 1.1000000000000003\n",
            "Episode: 161, Reward: 2.099999999999999\n",
            "Episode: 162, Reward: 1.1000000000000003\n",
            "Episode: 163, Reward: 4.100000000000001\n",
            "Episode: 164, Reward: 1.1000000000000003\n",
            "Episode: 165, Reward: 1.1\n",
            "Episode: 166, Reward: 2.099999999999999\n",
            "Episode: 167, Reward: 1.1\n",
            "Episode: 168, Reward: 1.1\n",
            "Episode: 169, Reward: 1.1000000000000003\n",
            "Episode: 170, Reward: 1.1\n",
            "Episode: 171, Reward: 2.099999999999999\n",
            "Episode: 172, Reward: 1.1\n",
            "Episode: 173, Reward: 2.099999999999999\n",
            "Episode: 174, Reward: 2.1\n",
            "Episode: 175, Reward: 1.1\n",
            "Episode: 176, Reward: 1.1000000000000003\n",
            "Episode: 177, Reward: 2.0999999999999996\n",
            "Episode: 178, Reward: 1.1000000000000003\n",
            "Episode: 179, Reward: 2.099999999999999\n",
            "Episode: 180, Reward: 2.099999999999999\n",
            "Episode: 181, Reward: 2.099999999999999\n",
            "Episode: 182, Reward: 1.1000000000000003\n",
            "Episode: 183, Reward: 2.0999999999999996\n",
            "Episode: 184, Reward: 2.099999999999999\n",
            "Episode: 185, Reward: 1.1000000000000003\n",
            "Episode: 186, Reward: 1.1000000000000003\n",
            "Episode: 187, Reward: 2.099999999999999\n",
            "Episode: 188, Reward: 1.0999999999999999\n",
            "Episode: 189, Reward: 1.1\n",
            "Episode: 190, Reward: 2.099999999999999\n",
            "Episode: 191, Reward: 1.1\n",
            "Episode: 192, Reward: 2.1000000000000005\n",
            "Episode: 193, Reward: 2.0999999999999996\n",
            "Episode: 194, Reward: 1.1000000000000003\n",
            "Episode: 195, Reward: 2.0999999999999996\n",
            "Episode: 196, Reward: 2.1\n",
            "Episode: 197, Reward: 1.1\n",
            "Episode: 198, Reward: 1.1\n",
            "Episode: 199, Reward: 2.099999999999999\n",
            "Episode: 200, Reward: 0.10000000000000003\n",
            "Episode: 201, Reward: 3.0999999999999996\n",
            "Episode: 202, Reward: 2.099999999999999\n",
            "Episode: 203, Reward: 1.1\n",
            "Episode: 204, Reward: 1.1\n",
            "Episode: 205, Reward: 1.1000000000000003\n",
            "Episode: 206, Reward: 1.1000000000000003\n",
            "Episode: 207, Reward: 2.099999999999999\n",
            "Episode: 208, Reward: 1.1\n",
            "Episode: 209, Reward: 1.1000000000000003\n",
            "Episode: 210, Reward: 1.1000000000000003\n",
            "Episode: 211, Reward: 2.099999999999999\n",
            "Episode: 212, Reward: 2.099999999999999\n",
            "Episode: 213, Reward: 1.1000000000000003\n",
            "Episode: 214, Reward: 2.1000000000000005\n",
            "Episode: 215, Reward: 2.099999999999999\n",
            "Episode: 216, Reward: 1.1\n",
            "Episode: 217, Reward: 1.1000000000000003\n",
            "Episode: 218, Reward: 1.1\n",
            "Episode: 219, Reward: 2.1\n",
            "Episode: 220, Reward: 3.099999999999999\n",
            "Episode: 221, Reward: 1.1000000000000003\n",
            "Episode: 222, Reward: 1.1000000000000003\n",
            "Episode: 223, Reward: 1.1000000000000003\n",
            "Episode: 224, Reward: 2.1\n",
            "Episode: 225, Reward: 1.1\n",
            "Episode: 226, Reward: 2.099999999999999\n",
            "Episode: 227, Reward: 1.1\n",
            "Episode: 228, Reward: 1.1000000000000003\n",
            "Episode: 229, Reward: 3.099999999999999\n",
            "Episode: 230, Reward: 1.1\n",
            "Episode: 231, Reward: 1.1000000000000005\n",
            "Episode: 232, Reward: 1.1000000000000003\n",
            "Episode: 233, Reward: 1.0999999999999999\n",
            "Episode: 234, Reward: 3.099999999999999\n",
            "Episode: 235, Reward: 1.1000000000000003\n",
            "Episode: 236, Reward: 1.1000000000000003\n",
            "Episode: 237, Reward: 2.099999999999999\n",
            "Episode: 238, Reward: 1.1000000000000003\n",
            "Episode: 239, Reward: 2.099999999999999\n",
            "Episode: 240, Reward: 2.099999999999999\n",
            "Episode: 241, Reward: 2.099999999999999\n",
            "Episode: 242, Reward: 1.1000000000000003\n",
            "Episode: 243, Reward: -0.8999999999999999\n",
            "Episode: 244, Reward: 1.1000000000000003\n",
            "Episode: 245, Reward: 1.1\n",
            "Episode: 246, Reward: 1.1000000000000003\n",
            "Episode: 247, Reward: 1.1\n",
            "Episode: 248, Reward: 1.1000000000000003\n",
            "Episode: 249, Reward: 1.1000000000000003\n",
            "Episode: 250, Reward: 1.1\n",
            "Episode: 251, Reward: 2.099999999999999\n",
            "Episode: 252, Reward: 1.1000000000000003\n",
            "Episode: 253, Reward: 1.1\n",
            "Episode: 254, Reward: 1.1000000000000003\n",
            "Episode: 255, Reward: 1.1\n",
            "Episode: 256, Reward: 1.1000000000000003\n",
            "Episode: 257, Reward: 1.1\n",
            "Episode: 258, Reward: 1.1000000000000003\n",
            "Episode: 259, Reward: 1.1000000000000003\n",
            "Episode: 260, Reward: 1.1000000000000003\n",
            "Episode: 261, Reward: 1.1000000000000008\n",
            "Episode: 262, Reward: 1.0999999999999996\n",
            "Episode: 263, Reward: 0.0999999999999999\n",
            "Episode: 264, Reward: 2.099999999999999\n",
            "Episode: 265, Reward: 2.099999999999999\n",
            "Episode: 266, Reward: 1.1000000000000003\n",
            "Episode: 267, Reward: 2.099999999999999\n",
            "Episode: 268, Reward: 1.1000000000000003\n",
            "Episode: 269, Reward: 0.09999999999999998\n",
            "Episode: 270, Reward: 3.099999999999999\n",
            "Episode: 271, Reward: 1.1000000000000003\n",
            "Episode: 272, Reward: 1.1000000000000005\n",
            "Episode: 273, Reward: 2.099999999999999\n",
            "Episode: 274, Reward: 1.1000000000000003\n",
            "Episode: 275, Reward: 1.1\n",
            "Episode: 276, Reward: 1.1000000000000003\n",
            "Episode: 277, Reward: 1.1000000000000003\n",
            "Episode: 278, Reward: 1.1000000000000003\n",
            "Episode: 279, Reward: 1.1000000000000003\n",
            "Episode: 280, Reward: 2.1\n",
            "Episode: 281, Reward: 1.1\n",
            "Episode: 282, Reward: 1.1000000000000003\n",
            "Episode: 283, Reward: 1.1000000000000003\n",
            "Episode: 284, Reward: 1.1000000000000003\n",
            "Episode: 285, Reward: 1.0999999999999999\n",
            "Episode: 286, Reward: -0.8999999999999999\n",
            "Episode: 287, Reward: 1.1000000000000003\n",
            "Episode: 288, Reward: 2.099999999999999\n",
            "Episode: 289, Reward: 1.1\n",
            "Episode: 290, Reward: 1.1\n",
            "Episode: 291, Reward: 2.0999999999999996\n",
            "Episode: 292, Reward: 1.1000000000000003\n",
            "Episode: 293, Reward: 1.0999999999999999\n",
            "Episode: 294, Reward: 1.1000000000000003\n",
            "Episode: 295, Reward: 1.1\n",
            "Episode: 296, Reward: 1.1000000000000003\n",
            "Episode: 297, Reward: 2.0999999999999996\n",
            "Episode: 298, Reward: 1.1000000000000003\n",
            "Episode: 299, Reward: 1.1000000000000003\n",
            "Episode: 300, Reward: 1.1\n",
            "Episode: 301, Reward: -0.9000000000000001\n",
            "Episode: 302, Reward: 1.0999999999999996\n",
            "Episode: 303, Reward: 1.1\n",
            "Episode: 304, Reward: 2.099999999999999\n",
            "Episode: 305, Reward: 1.1\n",
            "Episode: 306, Reward: 1.1000000000000003\n",
            "Episode: 307, Reward: 1.1000000000000003\n",
            "Episode: 308, Reward: 1.1000000000000003\n",
            "Episode: 309, Reward: 1.0999999999999992\n",
            "Episode: 310, Reward: 1.1000000000000005\n",
            "Episode: 311, Reward: 2.0999999999999996\n",
            "Episode: 312, Reward: 2.099999999999999\n",
            "Episode: 313, Reward: 2.099999999999999\n",
            "Episode: 314, Reward: 1.0999999999999999\n",
            "Episode: 315, Reward: 1.1\n",
            "Episode: 316, Reward: 1.1000000000000003\n",
            "Episode: 317, Reward: 1.1\n",
            "Episode: 318, Reward: 0.09999999999999998\n",
            "Episode: 319, Reward: 1.1000000000000003\n",
            "Episode: 320, Reward: 2.0999999999999996\n",
            "Episode: 321, Reward: 2.099999999999999\n",
            "Episode: 322, Reward: 1.1\n",
            "Episode: 323, Reward: 1.1\n",
            "Episode: 324, Reward: 1.1\n",
            "Episode: 325, Reward: 1.1000000000000003\n",
            "Episode: 326, Reward: 1.1000000000000003\n",
            "Episode: 327, Reward: 1.1\n",
            "Episode: 328, Reward: 1.1000000000000003\n",
            "Episode: 329, Reward: 2.0999999999999996\n",
            "Episode: 330, Reward: 1.1\n",
            "Episode: 331, Reward: -0.8999999999999999\n",
            "Episode: 332, Reward: 1.1000000000000003\n",
            "Episode: 333, Reward: 1.1\n",
            "Episode: 334, Reward: 2.099999999999999\n",
            "Episode: 335, Reward: 2.099999999999999\n",
            "Episode: 336, Reward: 2.099999999999999\n",
            "Episode: 337, Reward: 1.1000000000000003\n",
            "Episode: 338, Reward: 1.1000000000000003\n",
            "Episode: 339, Reward: 1.1\n",
            "Episode: 340, Reward: 1.1000000000000003\n",
            "Episode: 341, Reward: 1.1\n",
            "Episode: 342, Reward: 1.1\n",
            "Episode: 343, Reward: 1.0999999999999999\n",
            "Episode: 344, Reward: 2.0999999999999996\n",
            "Episode: 345, Reward: 2.099999999999999\n",
            "Episode: 346, Reward: 1.1000000000000003\n",
            "Episode: 347, Reward: 2.0999999999999996\n",
            "Episode: 348, Reward: 1.1000000000000005\n",
            "Episode: 349, Reward: 2.099999999999999\n",
            "Episode: 350, Reward: 1.1000000000000003\n",
            "Episode: 351, Reward: 3.099999999999999\n",
            "Episode: 352, Reward: 1.1000000000000003\n",
            "Episode: 353, Reward: 1.1\n",
            "Episode: 354, Reward: 1.0999999999999999\n",
            "Episode: 355, Reward: 2.0999999999999996\n",
            "Episode: 356, Reward: 2.099999999999999\n",
            "Episode: 357, Reward: 1.1\n",
            "Episode: 358, Reward: 1.1\n",
            "Episode: 359, Reward: 1.1000000000000003\n",
            "Episode: 360, Reward: 1.1000000000000003\n",
            "Episode: 361, Reward: 1.1000000000000003\n",
            "Episode: 362, Reward: 1.1000000000000003\n",
            "Episode: 363, Reward: 2.099999999999999\n",
            "Episode: 364, Reward: 1.1\n",
            "Episode: 365, Reward: 2.0999999999999996\n",
            "Episode: 366, Reward: 2.0999999999999996\n",
            "Episode: 367, Reward: 2.1\n",
            "Episode: 368, Reward: 2.0999999999999996\n",
            "Episode: 369, Reward: 2.1000000000000005\n",
            "Episode: 370, Reward: 1.1\n",
            "Episode: 371, Reward: 1.1000000000000003\n",
            "Episode: 372, Reward: 1.1000000000000005\n",
            "Episode: 373, Reward: 1.1\n",
            "Episode: 374, Reward: 1.1\n",
            "Episode: 375, Reward: 1.1\n",
            "Episode: 376, Reward: 2.099999999999999\n",
            "Episode: 377, Reward: 3.099999999999999\n",
            "Episode: 378, Reward: 2.0999999999999996\n",
            "Episode: 379, Reward: 2.1\n",
            "Episode: 380, Reward: 2.0999999999999996\n",
            "Episode: 381, Reward: 1.1000000000000003\n",
            "Episode: 382, Reward: 1.1000000000000003\n",
            "Episode: 383, Reward: 1.1000000000000003\n",
            "Episode: 384, Reward: 1.1\n",
            "Episode: 385, Reward: 1.1\n",
            "Episode: 386, Reward: 1.1000000000000003\n",
            "Episode: 387, Reward: 1.1\n",
            "Episode: 388, Reward: 1.1000000000000003\n",
            "Episode: 389, Reward: 2.099999999999999\n",
            "Episode: 390, Reward: 2.099999999999999\n",
            "Episode: 391, Reward: 1.1\n",
            "Episode: 392, Reward: 1.1\n",
            "Episode: 393, Reward: 3.0999999999999996\n",
            "Episode: 394, Reward: 3.099999999999999\n",
            "Episode: 395, Reward: 2.099999999999999\n",
            "Episode: 396, Reward: 1.1\n",
            "Episode: 397, Reward: 1.1000000000000003\n",
            "Episode: 398, Reward: 1.1\n",
            "Episode: 399, Reward: 3.099999999999999\n",
            "Episode: 400, Reward: 1.1\n",
            "Episode: 401, Reward: 2.1\n",
            "Episode: 402, Reward: 1.1000000000000003\n",
            "Episode: 403, Reward: 1.1000000000000003\n",
            "Episode: 404, Reward: 2.0999999999999996\n",
            "Episode: 405, Reward: 1.1\n",
            "Episode: 406, Reward: 2.0999999999999996\n",
            "Episode: 407, Reward: 1.0999999999999999\n",
            "Episode: 408, Reward: 1.1\n",
            "Episode: 409, Reward: 1.0999999999999999\n",
            "Episode: 410, Reward: 1.1000000000000003\n",
            "Episode: 411, Reward: 1.1\n",
            "Episode: 412, Reward: 1.1000000000000003\n",
            "Episode: 413, Reward: 4.1\n",
            "Episode: 414, Reward: 1.1000000000000003\n",
            "Episode: 415, Reward: 2.0999999999999996\n",
            "Episode: 416, Reward: 1.1\n",
            "Episode: 417, Reward: 1.0999999999999999\n",
            "Episode: 418, Reward: 2.1\n",
            "Episode: 419, Reward: 1.1000000000000003\n",
            "Episode: 420, Reward: 2.099999999999999\n",
            "Episode: 421, Reward: 1.1\n",
            "Episode: 422, Reward: 1.1000000000000003\n",
            "Episode: 423, Reward: 2.0999999999999996\n",
            "Episode: 424, Reward: 1.1000000000000003\n",
            "Episode: 425, Reward: 1.1\n",
            "Episode: 426, Reward: 1.1000000000000003\n",
            "Episode: 427, Reward: 3.0999999999999996\n",
            "Episode: 428, Reward: 1.1000000000000003\n",
            "Episode: 429, Reward: 1.1\n",
            "Episode: 430, Reward: 2.1\n",
            "Episode: 431, Reward: 2.099999999999999\n",
            "Episode: 432, Reward: 2.099999999999999\n",
            "Episode: 433, Reward: 2.0999999999999996\n",
            "Episode: 434, Reward: 1.1\n",
            "Episode: 435, Reward: 1.1\n",
            "Episode: 436, Reward: 1.1\n",
            "Episode: 437, Reward: 3.099999999999999\n",
            "Episode: 438, Reward: 2.0999999999999996\n",
            "Episode: 439, Reward: 2.1\n",
            "Episode: 440, Reward: 2.099999999999999\n",
            "Episode: 441, Reward: 2.0999999999999996\n",
            "Episode: 442, Reward: 2.099999999999999\n",
            "Episode: 443, Reward: 2.099999999999999\n",
            "Episode: 444, Reward: 1.1\n",
            "Episode: 445, Reward: 1.0999999999999999\n",
            "Episode: 446, Reward: 1.1000000000000003\n",
            "Episode: 447, Reward: 1.1000000000000003\n",
            "Episode: 448, Reward: 2.099999999999999\n",
            "Episode: 449, Reward: 2.0999999999999996\n",
            "Episode: 450, Reward: 2.1\n",
            "Episode: 451, Reward: 0.09999999999999998\n",
            "Episode: 452, Reward: 1.1\n",
            "Episode: 453, Reward: 1.0999999999999996\n",
            "Episode: 454, Reward: 1.1\n",
            "Episode: 455, Reward: 2.099999999999999\n",
            "Episode: 456, Reward: 1.1000000000000003\n",
            "Episode: 457, Reward: 1.1000000000000003\n",
            "Episode: 458, Reward: 1.1000000000000003\n",
            "Episode: 459, Reward: 2.1000000000000005\n",
            "Episode: 460, Reward: 1.1000000000000003\n",
            "Episode: 461, Reward: 1.1\n",
            "Episode: 462, Reward: 1.1\n",
            "Episode: 463, Reward: 1.1000000000000008\n",
            "Episode: 464, Reward: 2.099999999999999\n",
            "Episode: 465, Reward: 1.0999999999999996\n",
            "Episode: 466, Reward: 1.1\n",
            "Episode: 467, Reward: 3.0999999999999996\n",
            "Episode: 468, Reward: 1.1000000000000003\n",
            "Episode: 469, Reward: 1.1\n",
            "Episode: 470, Reward: -0.9000000000000001\n",
            "Episode: 471, Reward: 1.1000000000000003\n",
            "Episode: 472, Reward: 3.0999999999999996\n",
            "Episode: 473, Reward: 1.1000000000000003\n",
            "Episode: 474, Reward: 1.0999999999999999\n",
            "Episode: 475, Reward: 1.1\n",
            "Episode: 476, Reward: 1.1\n",
            "Episode: 477, Reward: 1.1000000000000003\n",
            "Episode: 478, Reward: 2.099999999999999\n",
            "Episode: 479, Reward: 2.099999999999999\n",
            "Episode: 480, Reward: 1.1000000000000003\n",
            "Episode: 481, Reward: 1.1\n",
            "Episode: 482, Reward: 1.1000000000000003\n",
            "Episode: 483, Reward: 1.1000000000000003\n",
            "Episode: 484, Reward: 2.099999999999999\n",
            "Episode: 485, Reward: 2.1\n",
            "Episode: 486, Reward: 1.1\n",
            "Episode: 487, Reward: 1.1000000000000003\n",
            "Episode: 488, Reward: 2.099999999999999\n",
            "Episode: 489, Reward: 1.1\n",
            "Episode: 490, Reward: 1.1\n",
            "Episode: 491, Reward: 1.1000000000000003\n",
            "Episode: 492, Reward: 1.1000000000000003\n",
            "Episode: 493, Reward: 1.1000000000000003\n",
            "Episode: 494, Reward: 2.099999999999999\n",
            "Episode: 495, Reward: 1.1\n",
            "Episode: 496, Reward: 1.1000000000000003\n",
            "Episode: 497, Reward: 2.0999999999999996\n",
            "Episode: 498, Reward: 1.1000000000000003\n",
            "Episode: 499, Reward: -0.8999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65YKhrBbuN5f"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UkrA8WpIwm_N"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xeR3Ie-8L3nT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e1ELPwXywt6u"
      },
      "execution_count": 26,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsV2zyUpuJ20H0JYKcXxWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}